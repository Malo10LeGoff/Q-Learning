{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requis\n",
    "\n",
    "Le but du jeu est ici d'entrainer un agent à remplir une caisse de 4x3x4 avec des palettes de 2 cases par une case.\n",
    "\n",
    "L'algorithme proposé est un algorithme de renforcement Q-learning\n",
    "\n",
    "Installer `gym`, `setuptool` et l'environnement `Pallet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.16.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.17.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (41.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/LENOVO/Projet%20RL\n",
      "Requirement already satisfied: gym in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym-pallet==0.0.1) (0.15.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym-pallet==0.0.1) (1.16.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym->gym-pallet==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym->gym-pallet==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym->gym-pallet==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym->gym-pallet==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym->gym-pallet==0.0.1) (0.17.1)\n",
      "Installing collected packages: gym-pallet\n",
      "  Found existing installation: gym-pallet 0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 407, in run\n",
      "    use_user_site=options.use_user_site,\n",
      "  File \"C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 51, in install_given_reqs\n",
      "    auto_confirm=True\n",
      "  File \"C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 830, in uninstall\n",
      "    uninstalled_pathset = UninstallPathSet.from_dist(dist)\n",
      "  File \"C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 538, in from_dist\n",
      "    '(at %s)' % (link_pointer, dist.project_name, dist.location)\n",
      "AssertionError: Egg-link c:\\users\\lenovo\\untitled folder does not match installed location of gym-pallet (at c:\\users\\lenovo\\projet rl)\n"
     ]
    }
   ],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le code commenté\n",
    "Premièrement, gérer les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, charger l'environnment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_pallet:pallet-v0') \n",
    "## on définit ici l'environnment avec le nombre de cases, d'états, les règles (quand est ce qu'on perd), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode0finished after 6 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode1finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode2finished after 8 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode3finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode4finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode5finished after 7 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode6finished after 2 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode7finished after 6 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode8finished after 2 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode9finished after 2 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode10finished after 5 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode11finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode12finished after 4 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode13finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode14finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode15finished after 12 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode16finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode17finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode18finished after 9 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode19finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode20finished after 4 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode21finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode22finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode23finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode24finished after 9 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode25finished after 3 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode26finished after 9 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode27finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode28finished after 12 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode29finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode30finished after 10 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode31finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode32finished after 11 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode33finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode34finished after 8 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode35finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode36finished after 5 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode37finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode38finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode39finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode40finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode41finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode42finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode43finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode44finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode45finished after 14 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode46finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode47finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode48finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode49finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode50finished after 14 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Episode51finished after 13 timesteps\n",
      "State [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "state :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-98489bdf38f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m## on entre dans la partie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                     \u001b[1;31m# rajoute l'état appris à la matrice d'état si on ne le connait pas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projet RL\\gym_pallet\\envs\\pallet_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## hyperparamètres\n",
    "\n",
    "discount_factor = 1.0 # grandeur représentant combien le modèle se soucie des récompenses éloignées dans le temps\n",
    "alpha = 0.6   # taux d'apprentissage\n",
    "epsilon = 0.1  # grandeur représentant le compromis entre l'exploration et l'exploitation\n",
    "EPSILON = [0.05,0.1,0.15,0.2,0.25,0.3,0.35]\n",
    "ALPHA = [0.6,0.65,0.7,0.75,0.8,0.85,0.9]\n",
    "DISCOUNT_FACTOR = [0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY_RATE = 0.0005   # taux de décroissance d'espilon\n",
    "\n",
    "state_size = 1000000\n",
    "action_size = 125\n",
    "\n",
    "  \n",
    "## Caracteristiques du problème    \n",
    "\n",
    "def maxListe(L):\n",
    "    \"\"\"\n",
    "    Retourne le maximum d'une liste\n",
    "    \"\"\"\n",
    "    a = L[0]\n",
    "    for i in range(0,len(L)):\n",
    "        if L[i]>a:\n",
    "            a=L[i]\n",
    "    return a\n",
    "    \n",
    "\n",
    "def Converter(action): \n",
    "    \"\"\"\n",
    "    Convertit une action dictionnaire en liste pour pouvoir la stocker dans matrice_action\n",
    "    \"\"\"\n",
    "    M=[]\n",
    "    M.append(action['ori'])\n",
    "    M.append(action['pos_x'])\n",
    "    M.append(action['pos_y'])\n",
    "    return M\n",
    "\n",
    "def DeConverter(L): \n",
    "    \"\"\"\n",
    "    Convertit la liste correspondant à une action en dictionnaire correspondant à l'action originale\n",
    "    \"\"\"\n",
    "    action = collections.OrderedDict()\n",
    "    action['ori'] = L[0]\n",
    "    action['pos_x'] = L[1]\n",
    "    action['pos_y'] = L[2]\n",
    "    return action\n",
    "\n",
    "\n",
    "def action_choice(Q,Matrice_état,Matrice_action,espilon,state):\n",
    "    \"\"\"\n",
    "    Choisit l'action à effectuer\n",
    "    \"\"\"\n",
    "    x = rd.random()  # nombre random entre 0 et 1\n",
    "    if x < epsilon :  # Exploration : on choisit une action au hasard\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        Action = Converter(action)\n",
    "        \n",
    "        if Action not in Matrice_action :\n",
    "            Matrice_action.append(Action)\n",
    "            \n",
    "    else : # on choisit l'action pour laquelle Q[index_state][action] est le plus grand avec np.argmax\n",
    "        \n",
    "        index_state = Matrice_état.index(state)\n",
    "        \n",
    "        if np.argmax(Q[index_state]) != 0 :\n",
    "            \n",
    "            index_best_action = np.argmax(Q[index_state])   # np.argmax renvoie l'indice de la colonne du plus grand élément de la ligne index_state\n",
    "            \n",
    "            Action = Matrice_action[index_best_action]  # on retrouve l'action qui donne la plus grande récompense en la cherchant dans la liste d'action\n",
    "            \n",
    "            action = DeConverter(Action)  # on la reconvertit en dictionnaire pour que la fonction step la reconaisse\n",
    "            \n",
    "        else :    ## si on ne connait pas encore l'état alors on choisit une action au hasard\n",
    "            \n",
    "            action = env.action_space.sample()\n",
    "            Action = Converter(action)\n",
    "            if Action not in Matrice_action :\n",
    "                Matrice_action.append(Action)\n",
    "    return action\n",
    "\n",
    "\n",
    "## Phase d'entrainement\n",
    "            \n",
    "for epsilon in EPSILON :\n",
    "    \n",
    "    for alpha in ALPHA :\n",
    "        \n",
    "        for discount_factor in DISCOUNT_FACTOR :\n",
    "            \n",
    "            Q = np.zeros((state_size,action_size))  ## Q_table du problème, Q[s][a] correspond à la récompense attendue si on applique l'action a à l'état s\n",
    "            Matrice_état = []   ## matrice contenant les états connus\n",
    "            Matrice_action = [] ## matrice contenant les actions connues\n",
    "\n",
    "            \n",
    "            REWARD = []\n",
    "            for i_episode in range(125): #  L'apprentissage d'un modèle consiste en 125 tests sucessifs, on fait 125 parties\n",
    "                \n",
    "                State = env.reset() # a chaque test on réinitialise l'environnement, ici State correspond à l'état initial\n",
    "\n",
    "\n",
    "                ## Transformation de l'état en une liste de 9 nombres que l'on va rajouter ensuite dans la matrice d'état\n",
    "                ## Autrement dit on représente l'état de notre palette par une liste de 9 éléments qui correspond aux lignes du dictionnaire mise bout à bout\n",
    "                State = State['fill'] \n",
    "                \n",
    "                state = []\n",
    "                state.extend(State[0])\n",
    "                state.extend(State[1])\n",
    "                state.extend(State[2])\n",
    "                \n",
    "\n",
    "                total_reward = 0 \n",
    "\n",
    "                done = False\n",
    "                t = 0\n",
    "\n",
    "                while not done:   ## on entre dans la partie\n",
    "                    t += 1\n",
    "                    env.render()\n",
    "\n",
    "                    # rajoute l'état appris à la matrice d'état si on ne le connait pas\n",
    "\n",
    "                    if state not in Matrice_état :\n",
    "                        Matrice_état.append(state)\n",
    "                    \n",
    "                    # Indice de l'état dans la matrice état\n",
    "                    index_state = Matrice_état.index(state)\n",
    "                    #print('state : ',state)\n",
    "\n",
    "                    # Choix de l'action\n",
    "                    action = action_choice(Q,Matrice_état,Matrice_action,epsilon,state) \n",
    "\n",
    "                    # Indice de l'action dans la matrice d'action\n",
    "                    Action = Converter(action)    \n",
    "                    index_action = Matrice_action.index(Action)\n",
    "\n",
    "                    # Transition à l'état suivant \n",
    "                    Next_state, reward, done, _ = env.step(action) # reward correspond à +1 à chaque tour qui passe c'est à dire à chaque fois qu'on rajoute une caisse, peut on la modifier pour que ce soit plus précis et inclure les actions à ne vraiment pas prendre comme mettre une caisse en dehors \n",
    "\n",
    "                   \n",
    "                    total_reward = total_reward+reward\n",
    "\n",
    "                    # transformation de l'état en liste que l'on rajoute à la matrice d'état\n",
    "\n",
    "                    Next_state = Next_state['fill']\n",
    "                    next_state = []\n",
    "                    next_state.extend(Next_state[0])\n",
    "                    next_state.extend(Next_state[1])\n",
    "                    next_state.extend(Next_state[2])\n",
    "                    \n",
    "                    if next_state not in Matrice_état :\n",
    "                        Matrice_état.append(next_state)\n",
    "\n",
    "\n",
    "                    index_next_state = Matrice_état.index(next_state)\n",
    "\n",
    "                    # corrige Q_table\n",
    "                    best_next_action = np.argmax(Q[index_next_state])   # choisit la meilleure action pour l'état suivant, retourne l'indice de la meilleure action\n",
    "                    td_target = reward + discount_factor * Q[index_next_state][best_next_action] # max(Q(s',a'))\n",
    "                    td_delta = td_target - Q[index_state][index_action] # Q(s,a), ici on fait la différence entre la valeur \"optimale\" et Q(s,a), cette différence va tendre vers 0 au fur et à mesure des itérations\n",
    "                    Q[index_state][index_action] += alpha * td_delta \n",
    "\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        env.render()\n",
    "                        REWARD = REWARD + [total_reward]\n",
    "                        print(\"Episode\"+str(i_episode)+\"finished after {} timesteps\".format(t + 1))\n",
    "                        break\n",
    "\n",
    "                epsilon = MIN_EPSILON + (epsilon - MIN_EPSILON) * np.exp(-DECAY_RATE * i_episode) # réduction d'espilon au fur et à mesure des parties afin de réduire de plus en plus l'exploration\n",
    "            print(alpha,discount_factor,epsilon, maxListe(REWARD))\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
